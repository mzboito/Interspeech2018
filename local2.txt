04/24 00:39:00 label: FR->MB local attention version
04/24 00:39:00 description:
  second model of the local2global segmentation
04/24 00:39:00 /home/getalp/zanonbom/seq2seq/translate/__main__.py ../NMT_experiments/MBOSHI/local2global/local/exp2/config.yaml --train -v
04/24 00:39:00 commit hash d7914bbb0f1dc22438de0e15b82ebec43e0614ff
04/24 00:39:00 tensorflow version: 1.2.0-rc1
04/24 00:39:00 program arguments
04/24 00:39:00   aggregation_method   'concat'
04/24 00:39:00   align_encoder_id     0
04/24 00:39:00   allow_growth         True
04/24 00:39:00   att_window_size      0
04/24 00:39:00   attention_type       'local'
04/24 00:39:00   attn_filter_length   0
04/24 00:39:00   attn_filters         0
04/24 00:39:00   attn_prev_word       False
04/24 00:39:00   attn_size            32
04/24 00:39:00   attn_temperature     1
04/24 00:39:00   attn_window_size     0
04/24 00:39:00   average              False
04/24 00:39:00   baseline_activation  None
04/24 00:39:00   baseline_learning_rate 0.001
04/24 00:39:00   baseline_optimizer   'adam'
04/24 00:39:00   baseline_steps       0
04/24 00:39:00   batch_mode           'standard'
04/24 00:39:00   batch_size           32
04/24 00:39:00   beam_size            1
04/24 00:39:00   bidir                True
04/24 00:39:00   bidir_projection     False
04/24 00:39:00   binary               False
04/24 00:39:00   cell_size            32
04/24 00:39:00   cell_type            'LSTM'
04/24 00:39:00   character_level      False
04/24 00:39:00   checkpoints          []
04/24 00:39:00   conditional_rnn      False
04/24 00:39:00   config               '../NMT_experiments/MBOSHI/local2global/local/exp2/config.yaml'
04/24 00:39:00   convolutions         None
04/24 00:39:00   data_dir             '../NMT_experiments/MBOSHI/local2global/files/'
04/24 00:39:00   debug                False
04/24 00:39:00   decay_after_n_epoch  0
04/24 00:39:00   decay_every_n_epoch  None
04/24 00:39:00   decay_if_no_progress None
04/24 00:39:00   decoders             [{'name': 'mb'}]
04/24 00:39:00   description          'second model of the local2global segmentation'
04/24 00:39:00   dev_prefix           ['dev', 'train']
04/24 00:39:00   embedding_dropout    0.0
04/24 00:39:00   embedding_initializer None
04/24 00:39:00   embedding_size       32
04/24 00:39:01   embedding_weight_scale None
04/24 00:39:01   encoders             [{'name': 'fr'}]
04/24 00:39:01   ensemble             False
04/24 00:39:01   eval_burn_in         0
04/24 00:39:01   feed_previous        0.0
04/24 00:39:01   final_state          'last'
04/24 00:39:01   freeze_variables     []
04/24 00:39:01   generate_first       True
04/24 00:39:01   gpu_id               0
04/24 00:39:01   highway_layers       0
04/24 00:39:01   initial_state_dropout 0.5
04/24 00:39:01   initializer          None
04/24 00:39:01   input_layer_dropout  0.0
04/24 00:39:01   input_layers         None
04/24 00:39:01   keep_best            4
04/24 00:39:01   keep_every_n_hours   0
04/24 00:39:01   label                'FR->MB local attention version'
04/24 00:39:01   layer_norm           False
04/24 00:39:01   layers               1
04/24 00:39:01   learning_rate        0.001
04/24 00:39:01   learning_rate_decay_factor 1.0
04/24 00:39:01   len_normalization    1.0
04/24 00:39:01   log_file             'local_log.txt'
04/24 00:39:01   loss_function        'xent'
04/24 00:39:01   max_dev_size         0
04/24 00:39:01   max_epochs           0
04/24 00:39:01   max_gradient_norm    5.0
04/24 00:39:01   max_len              84
04/24 00:39:01   max_steps            250000
04/24 00:39:01   max_test_size        0
04/24 00:39:01   max_to_keep          1
04/24 00:39:01   max_train_size       0
04/24 00:39:01   maxout_stride        None
04/24 00:39:01   mem_fraction         1.0
04/24 00:39:01   min_learning_rate    1e-06
04/24 00:39:01   model_dir            '../NMT_experiments/MBOSHI/local2global/local/exp2/model/'
04/24 00:39:01   moving_average       None
04/24 00:39:01   no_gpu               False
04/24 00:39:01   optimizer            'adam'
04/24 00:39:01   orthogonal_init      False
04/24 00:39:01   output               None
04/24 00:39:01   output_dropout       0.0
04/24 00:39:01   parallel_iterations  16
04/24 00:39:01   pervasive_dropout    False
04/24 00:39:01   pooling_avg          True
04/24 00:39:01   post_process_script  None
04/24 00:39:01   pred_deep_layer      False
04/24 00:39:01   pred_edits           False
04/24 00:39:01   pred_embed_proj      False
04/24 00:39:01   pred_maxout_layer    True
04/24 00:39:01   purge                False
04/24 00:39:01   raw_output           False
04/24 00:39:01   read_ahead           10
04/24 00:39:01   reconstruction_attn_weight 0.05
04/24 00:39:01   reconstruction_decoders False
04/24 00:39:01   reconstruction_weight 1.0
04/24 00:39:01   reinforce_after_n_epoch None
04/24 00:39:01   remove_unk           False
04/24 00:39:01   reverse              False
04/24 00:39:01   reverse_input        False
04/24 00:39:01   reward_function      'sentence_bleu'
04/24 00:39:01   rnn_feed_attn        True
04/24 00:39:01   rnn_input_dropout    0.5
04/24 00:39:01   rnn_output_dropout   0.0
04/24 00:39:01   rnn_state_dropout    0.0
04/24 00:39:01   save                 False
04/24 00:39:01   score_function       'corpus_bleu'
04/24 00:39:01   score_functions      ['bleu', 'loss']
04/24 00:39:01   script_dir           'scripts'
04/24 00:39:01   sgd_after_n_epoch    None
04/24 00:39:01   sgd_learning_rate    1.0
04/24 00:39:01   shuffle              True
04/24 00:39:01   softmax_temperature  1.0
04/24 00:39:01   steps_per_checkpoint 10000
04/24 00:39:01   steps_per_eval       10000
04/24 00:39:01   swap_memory          True
04/24 00:39:01   tie_embeddings       False
04/24 00:39:01   time_pooling         None
04/24 00:39:01   train                True
04/24 00:39:01   train_initial_states True
04/24 00:39:01   train_prefix         'train'
04/24 00:39:01   truncate_lines       True
04/24 00:39:01   update_first         False
04/24 00:39:01   use_baseline         False
04/24 00:39:01   use_dropout          True
04/24 00:39:01   use_lstm             True
04/24 00:39:01   use_lstm_full_state  False
04/24 00:39:01   use_previous_word    True
04/24 00:39:01   verbose              True
04/24 00:39:01   vocab_prefix         'vocab'
04/24 00:39:01   weight_scale         0.1
04/24 00:39:01   word_dropout         0.0
04/24 00:39:01 python random seed: 3181960164955844345
04/24 00:39:01 tf random seed:     6718791458026717279
04/24 00:39:01 creating model
04/24 00:39:01 using device: /gpu:0
04/24 00:39:01 copying vocab to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/data/vocab.fr
04/24 00:39:01 copying vocab to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/data/vocab.mb
04/24 00:39:01 reading vocabularies
04/24 00:39:01 creating model
04/24 00:39:07 model parameters (29)
04/24 00:39:07   baseline_step:0 ()
04/24 00:39:07   decoder_mb/attention_fr/U_a/kernel:0 (64, 32)
04/24 00:39:07   decoder_mb/attention_fr/W_a/bias:0 (32,)
04/24 00:39:07   decoder_mb/attention_fr/W_a/kernel:0 (32, 32)
04/24 00:39:07   decoder_mb/attention_fr/Wp:0 (32, 32)
04/24 00:39:07   decoder_mb/attention_fr/v_a:0 (32,)
04/24 00:39:07   decoder_mb/attention_fr/vp:0 (32, 1)
04/24 00:39:07   decoder_mb/basic_lstm_cell/bias:0 (128,)
04/24 00:39:07   decoder_mb/basic_lstm_cell/kernel:0 (128, 128)
04/24 00:39:07   decoder_mb/fr/initial_state_projection/bias:0 (64,)
04/24 00:39:07   decoder_mb/fr/initial_state_projection/kernel:0 (32, 64)
04/24 00:39:07   decoder_mb/maxout/bias:0 (32,)
04/24 00:39:07   decoder_mb/maxout/kernel:0 (128, 32)
04/24 00:39:07   decoder_mb/softmax1/bias:0 (39,)
04/24 00:39:07   decoder_mb/softmax1/kernel:0 (16, 39)
04/24 00:39:07   embedding_fr:0 (4923, 32)
04/24 00:39:07   embedding_mb:0 (39, 32)
04/24 00:39:07   encoder_fr/initial_state_bw:0 (64,)
04/24 00:39:07   encoder_fr/initial_state_fw:0 (64,)
04/24 00:39:07   encoder_fr/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/basic_lstm_cell/bias:0 (128,)
04/24 00:39:07   encoder_fr/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/basic_lstm_cell/kernel:0 (64, 128)
04/24 00:39:07   encoder_fr/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/basic_lstm_cell/bias:0 (128,)
04/24 00:39:07   encoder_fr/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/basic_lstm_cell/kernel:0 (64, 128)
04/24 00:39:07   global_step:0 ()
04/24 00:39:07   initial_state_keep_prob:0 ()
04/24 00:39:07   initial_state_keep_prob_1:0 ()
04/24 00:39:07   learning_rate:0 ()
04/24 00:39:07   rnn_input_keep_prob:0 ()
04/24 00:39:07   rnn_input_keep_prob_1:0 ()
04/24 00:39:07 number of parameters: 0.20M
04/24 00:39:10 global step: 0
04/24 00:39:10 baseline step: 0
04/24 00:39:10 reading training data
04/24 00:39:10 total line count: 4616
04/24 00:39:11 files: ../NMT_experiments/MBOSHI/local2global/files/train.fr ../NMT_experiments/MBOSHI/local2global/files/train.mb
04/24 00:39:11 lines reads: 4616
04/24 00:39:11 reading development data
04/24 00:39:11 files: ../NMT_experiments/MBOSHI/local2global/files/dev.fr ../NMT_experiments/MBOSHI/local2global/files/dev.mb
04/24 00:39:11 lines reads: 514
04/24 00:39:11 files: ../NMT_experiments/MBOSHI/local2global/files/train.fr ../NMT_experiments/MBOSHI/local2global/files/train.mb
04/24 00:39:11 lines reads: 4616
04/24 00:39:11 starting training
04/24 01:09:00 step 10000 epoch 70 learning rate 0.001 step-time 0.177 loss 54.703
04/24 01:09:00 starting evaluation
04/24 01:09:05 dev bleu=13.45 loss=53.30 penalty=0.919 ratio=0.922
04/24 01:09:54 train bleu=19.36 loss=44.28 penalty=0.952 ratio=0.953
04/24 01:09:54 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 01:09:55 finished saving model
04/24 01:09:55 new best model
04/24 01:39:44 step 20000 epoch 139 learning rate 0.001 step-time 0.177 loss 46.643
04/24 01:39:44 starting evaluation
04/24 01:39:49 dev bleu=14.54 loss=52.02 penalty=0.928 ratio=0.931
04/24 01:40:39 train bleu=23.32 loss=39.70 penalty=0.973 ratio=0.973
04/24 01:40:39 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 01:40:40 finished saving model
04/24 01:40:40 new best model
04/24 02:10:15 step 30000 epoch 208 learning rate 0.001 step-time 0.176 loss 44.284
04/24 02:10:15 starting evaluation
04/24 02:10:20 dev bleu=14.72 loss=51.83 penalty=0.876 ratio=0.883
04/24 02:11:08 train bleu=24.73 loss=37.45 penalty=0.926 ratio=0.929
04/24 02:11:09 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 02:11:09 finished saving model
04/24 02:11:09 new best model
04/24 02:40:45 step 40000 epoch 278 learning rate 0.001 step-time 0.176 loss 43.100
04/24 02:40:45 starting evaluation
04/24 02:40:50 dev bleu=15.22 loss=51.49 penalty=0.923 ratio=0.926
04/24 02:41:39 train bleu=26.42 loss=36.25 penalty=0.961 ratio=0.962
04/24 02:41:39 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 02:41:39 finished saving model
04/24 02:41:39 new best model
04/24 03:11:24 step 50000 epoch 347 learning rate 0.001 step-time 0.177 loss 42.317
04/24 03:11:25 starting evaluation
04/24 03:11:30 dev bleu=15.25 loss=51.16 penalty=0.936 ratio=0.938
04/24 03:12:20 train bleu=27.20 loss=35.42 penalty=0.979 ratio=0.979
04/24 03:12:20 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 03:12:21 finished saving model
04/24 03:12:21 new best model
04/24 03:41:57 step 60000 epoch 416 learning rate 0.001 step-time 0.176 loss 41.748
04/24 03:41:58 starting evaluation
04/24 03:42:03 dev bleu=15.33 loss=51.33 penalty=0.899 ratio=0.903
04/24 03:42:52 train bleu=27.28 loss=34.72 penalty=0.944 ratio=0.946
04/24 03:42:52 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 03:42:52 finished saving model
04/24 03:42:52 new best model
04/24 04:12:27 step 70000 epoch 486 learning rate 0.001 step-time 0.176 loss 41.321
04/24 04:12:27 starting evaluation
04/24 04:12:32 dev bleu=15.86 loss=51.02 penalty=0.947 ratio=0.948
04/24 04:13:22 train bleu=28.14 loss=34.28 penalty=0.984 ratio=0.984
04/24 04:13:22 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 04:13:23 finished saving model
04/24 04:13:23 new best model
04/24 04:42:58 step 80000 epoch 555 learning rate 0.001 step-time 0.176 loss 40.950
04/24 04:42:59 starting evaluation
04/24 04:43:04 dev bleu=15.64 loss=50.80 penalty=0.951 ratio=0.952
04/24 04:43:55 train bleu=28.87 loss=33.96 penalty=0.996 ratio=0.996
04/24 04:43:55 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 04:43:56 finished saving model
04/24 05:13:35 step 90000 epoch 624 learning rate 0.001 step-time 0.176 loss 40.669
04/24 05:13:36 starting evaluation
04/24 05:13:41 dev bleu=14.88 loss=51.40 penalty=0.876 ratio=0.883
04/24 05:14:29 train bleu=28.27 loss=33.61 penalty=0.922 ratio=0.925
04/24 05:14:29 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 05:14:30 finished saving model
04/24 05:44:22 step 100000 epoch 694 learning rate 0.001 step-time 0.178 loss 40.419
04/24 05:44:22 starting evaluation
04/24 05:44:27 dev bleu=15.11 loss=50.96 penalty=0.911 ratio=0.914
04/24 05:45:17 train bleu=28.68 loss=33.27 penalty=0.949 ratio=0.950
04/24 05:45:17 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 05:45:17 finished saving model
04/24 06:15:07 step 110000 epoch 763 learning rate 0.001 step-time 0.177 loss 40.217
04/24 06:15:07 starting evaluation
04/24 06:15:13 dev bleu=15.80 loss=51.26 penalty=0.909 ratio=0.913
04/24 06:16:03 train bleu=28.66 loss=33.02 penalty=0.945 ratio=0.946
04/24 06:16:03 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 06:16:03 finished saving model
04/24 06:45:39 step 120000 epoch 832 learning rate 0.001 step-time 0.176 loss 40.022
04/24 06:45:39 starting evaluation
04/24 06:45:44 dev bleu=15.71 loss=51.13 penalty=0.910 ratio=0.913
04/24 06:46:34 train bleu=29.14 loss=32.83 penalty=0.942 ratio=0.943
04/24 06:46:34 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 06:46:35 finished saving model
04/24 07:16:14 step 130000 epoch 902 learning rate 0.001 step-time 0.176 loss 39.862
04/24 07:16:14 starting evaluation
04/24 07:16:19 dev bleu=16.00 loss=51.24 penalty=0.907 ratio=0.911
04/24 07:17:09 train bleu=29.37 loss=32.67 penalty=0.940 ratio=0.942
04/24 07:17:09 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 07:17:09 finished saving model
04/24 07:17:09 new best model
04/24 07:46:43 step 140000 epoch 971 learning rate 0.001 step-time 0.176 loss 39.717
04/24 07:46:43 starting evaluation
04/24 07:46:48 dev bleu=16.31 loss=50.77 penalty=0.942 ratio=0.944
04/24 07:47:39 train bleu=29.87 loss=32.51 penalty=0.975 ratio=0.976
04/24 07:47:39 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 07:47:39 finished saving model
04/24 07:47:39 new best model
04/24 08:17:21 step 150000 epoch 1040 learning rate 0.001 step-time 0.177 loss 39.590
04/24 08:17:22 starting evaluation
04/24 08:17:27 dev bleu=16.03 loss=51.00 penalty=0.884 ratio=0.890
04/24 08:18:16 train bleu=29.48 loss=32.36 penalty=0.930 ratio=0.933
04/24 08:18:16 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 08:18:17 finished saving model
04/24 08:48:08 step 160000 epoch 1110 learning rate 0.001 step-time 0.177 loss 39.486
04/24 08:48:08 starting evaluation
04/24 08:48:14 dev bleu=16.09 loss=50.84 penalty=0.935 ratio=0.937
04/24 08:49:04 train bleu=30.05 loss=32.26 penalty=0.973 ratio=0.973
04/24 08:49:04 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 08:49:05 finished saving model
04/24 09:18:43 step 170000 epoch 1179 learning rate 0.001 step-time 0.176 loss 39.361
04/24 09:18:43 starting evaluation
04/24 09:18:49 dev bleu=15.82 loss=50.99 penalty=0.894 ratio=0.900
04/24 09:19:38 train bleu=29.53 loss=31.99 penalty=0.938 ratio=0.940
04/24 09:19:38 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 09:19:38 finished saving model
04/24 09:49:12 step 180000 epoch 1248 learning rate 0.001 step-time 0.176 loss 39.285
04/24 09:49:13 starting evaluation
04/24 09:49:18 dev bleu=15.78 loss=50.71 penalty=0.929 ratio=0.931
04/24 09:50:07 train bleu=29.94 loss=32.02 penalty=0.958 ratio=0.959
04/24 09:50:08 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 09:50:08 finished saving model
04/24 10:19:58 step 190000 epoch 1318 learning rate 0.001 step-time 0.177 loss 39.181
04/24 10:19:58 starting evaluation
04/24 10:20:03 dev bleu=16.05 loss=50.63 penalty=0.946 ratio=0.948
04/24 10:20:54 train bleu=30.00 loss=31.95 penalty=0.988 ratio=0.988
04/24 10:20:54 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 10:20:55 finished saving model
04/24 10:50:51 step 200000 epoch 1387 learning rate 0.001 step-time 0.178 loss 39.113
04/24 10:50:51 starting evaluation
04/24 10:50:56 dev bleu=16.15 loss=50.80 penalty=0.904 ratio=0.909
04/24 10:51:46 train bleu=30.00 loss=31.83 penalty=0.948 ratio=0.949
04/24 10:51:46 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 10:51:46 finished saving model
04/24 11:21:35 step 210000 epoch 1456 learning rate 0.001 step-time 0.177 loss 39.055
04/24 11:21:35 starting evaluation
04/24 11:21:40 dev bleu=16.14 loss=50.64 penalty=0.914 ratio=0.917
04/24 11:22:31 train bleu=30.22 loss=31.75 penalty=0.957 ratio=0.958
04/24 11:22:31 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 11:22:31 finished saving model
04/24 11:52:24 step 220000 epoch 1526 learning rate 0.001 step-time 0.178 loss 38.967
04/24 11:52:24 starting evaluation
04/24 11:52:29 dev bleu=16.04 loss=50.59 penalty=0.916 ratio=0.919
04/24 11:53:19 train bleu=30.14 loss=31.65 penalty=0.953 ratio=0.954
04/24 11:53:19 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 11:53:19 finished saving model
04/24 12:22:55 step 230000 epoch 1595 learning rate 0.001 step-time 0.176 loss 38.909
04/24 12:22:55 starting evaluation
04/24 12:23:00 dev bleu=16.01 loss=50.84 penalty=0.925 ratio=0.928
04/24 12:23:49 train bleu=30.20 loss=31.55 penalty=0.952 ratio=0.953
04/24 12:23:49 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 12:23:49 finished saving model
04/24 12:53:29 step 240000 epoch 1664 learning rate 0.001 step-time 0.176 loss 38.844
04/24 12:53:29 starting evaluation
04/24 12:53:34 dev bleu=16.59 loss=50.48 penalty=0.918 ratio=0.921
04/24 12:54:24 train bleu=30.38 loss=31.46 penalty=0.955 ratio=0.956
04/24 12:54:24 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 12:54:24 finished saving model
04/24 12:54:24 new best model
04/24 13:24:12 step 250000 epoch 1734 learning rate 0.001 step-time 0.177 loss 38.785
04/24 13:24:12 starting evaluation
04/24 13:24:18 dev bleu=15.74 loss=51.21 penalty=0.860 ratio=0.869
04/24 13:25:07 train bleu=29.69 loss=31.56 penalty=0.905 ratio=0.909
04/24 13:25:07 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 13:25:07 finished saving model
04/24 13:25:07 finished training
04/24 13:25:07 exiting...
04/24 13:25:07 saving model to ../NMT_experiments/MBOSHI/local2global/local/exp2/model/checkpoints
04/24 13:25:07 finished saving model
